{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color='blue'>Load MNIST Dataset</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='blue'>Get Training and Testing Sets</font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 11s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='blue'>Printing an image and labels</font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1  69  35\n",
      "    0   1   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0  34 160 128 191 114\n",
      "    0   0   0   0   0   0   0 107  59   0]\n",
      " [  0   0   0   0   0   1   1   0   0   0   5  48 126 151 160 172 113 130\n",
      "   61   0   0   0   0  51 201 245 166   0]\n",
      " [  1   2   1   0   0   0   0   0   0  30 164 166 136 119 101  88 165 149\n",
      "  141 116  97  92 132 162 141 233 210   0]\n",
      " [  0   0   0   0   2  14  34  64  82 119 135  88 136 122 206 190 122 122\n",
      "  116 144 145 151 144 140 151 159 182  11]\n",
      " [  0   2  42  72  90 105 115 102 106 120  93 149 131  99 185 198  94  94\n",
      "  105  97 113 102 114 123 107  93 147  31]\n",
      " [ 92 228 130 109  95  86  85  73  94  85 103 102  65  74  77  78  90  97\n",
      "  107 114 114 123 144 139 136 157 189  31]\n",
      " [ 19  88 141 173 173 173 161 170 203 198 205 215 224 227 230 245 255 252\n",
      "  248 252 239 228 216 214 214 208 231  82]\n",
      " [ 14   0   0   0   0   0   0   0   5   3  10   5   6  14  14  11   6   1\n",
      "    0   0   0   0   0   0   0   0   0  13]\n",
      " [  0  14  28  31  28  24  27  24  24  23  30  22  19  21  21  22  23  23\n",
      "   24  26  28  35  39  36  36  38  44  22]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(training_images[14])\n",
    "print(training_labels[14])\n",
    "print(training_images[14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='blue'>Normalizing</font></h2>\n",
    "<p>As all the values are between 0 and 255, we can normalize the values, putting everything </br>\n",
    "in terms of values between 1 and 0, dividing all the values by 255</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='blue'>Creating Model</font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sequential:** defines a SEQUENCE of layers in the neural network\n",
    "- **Flatten:** Takes the images (a square) and turn them into a 1 dimensional set.\n",
    "- **Dense:** Adds a layer of neurons, each layer of neurons needs an **activation function** to tell them what to do. \n",
    "    - **Relu** effectively means **if X > 0 return X, else return 0**, so what it does it only passes values 0 or greater to the next layer network.\n",
    "    - **Softmax** takes a set of values, and effectively picks the biggest one, so, for example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it saves you from fishing through it looking for the biggest value, and turns it into [0,0,0,0,1,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.0865 - acc: 0.9679\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0872 - acc: 0.9679\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.0836 - acc: 0.9681\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.0867 - acc: 0.9677\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0815 - acc: 0.9694\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0816 - acc: 0.9696\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0825 - acc: 0.9697\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0806 - acc: 0.9701\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0790 - acc: 0.9703\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0769 - acc: 0.9708\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0775 - acc: 0.9711\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0776 - acc: 0.9708\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0745 - acc: 0.9720\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0744 - acc: 0.9725\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0759 - acc: 0.9717\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0690 - acc: 0.9750\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0707 - acc: 0.9739\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0697 - acc: 0.9744\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0719 - acc: 0.9736\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0684 - acc: 0.9740\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0683 - acc: 0.9741\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0687 - acc: 0.9746\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0659 - acc: 0.9756\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0651 - acc: 0.9764\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0662 - acc: 0.9752\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0656 - acc: 0.9751\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0658 - acc: 0.9759\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0629 - acc: 0.9765\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0628 - acc: 0.9758\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0647 - acc: 0.9761\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0606 - acc: 0.9771\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0598 - acc: 0.9783\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0598 - acc: 0.9772\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0579 - acc: 0.9776\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0621 - acc: 0.9775\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0570 - acc: 0.9785\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0570 - acc: 0.9790\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0538 - acc: 0.9799\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0579 - acc: 0.9786\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0557 - acc: 0.9797\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0558 - acc: 0.9794\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0557 - acc: 0.9789\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0537 - acc: 0.9795\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0547 - acc: 0.9797\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0536 - acc: 0.9808\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0511 - acc: 0.9808\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0541 - acc: 0.9799\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0488 - acc: 0.9816\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0514 - acc: 0.9809\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0503 - acc: 0.9811\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0506 - acc: 0.9815\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0489 - acc: 0.9817\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0475 - acc: 0.9827\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0525 - acc: 0.9808\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0493 - acc: 0.9812\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0447 - acc: 0.9827\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0494 - acc: 0.9816\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0482 - acc: 0.9825\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0439 - acc: 0.9836\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0476 - acc: 0.9829\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0447 - acc: 0.9832\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0456 - acc: 0.9833\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0432 - acc: 0.9845\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0458 - acc: 0.9832\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0480 - acc: 0.9824\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0430 - acc: 0.9836\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0437 - acc: 0.9837\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0447 - acc: 0.9833\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0408 - acc: 0.9854\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0410 - acc: 0.9851\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0419 - acc: 0.9842\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0414 - acc: 0.9846\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0426 - acc: 0.9844\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0414 - acc: 0.9844\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0387 - acc: 0.9853\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0421 - acc: 0.9850\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0416 - acc: 0.9847\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0386 - acc: 0.9854\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.0422 - acc: 0.9846\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0426 - acc: 0.9842\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0392 - acc: 0.9856\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 8s 128us/sample - loss: 0.0378 - acc: 0.9858\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.0373 - acc: 0.9862\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0423 - acc: 0.9843\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0370 - acc: 0.9865\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0384 - acc: 0.9861\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0376 - acc: 0.9862\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0383 - acc: 0.9858\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0356 - acc: 0.9867\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0365 - acc: 0.9868\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0385 - acc: 0.9853\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.0362 - acc: 0.9866\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.0357 - acc: 0.9864\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0357 - acc: 0.9870\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0342 - acc: 0.9873\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0371 - acc: 0.9866\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0317 - acc: 0.9882\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0366 - acc: 0.9863\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0378 - acc: 0.9867\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0316 - acc: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c1b10b3c18>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 38us/sample - loss: 1.1161 - acc: 0.8810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1160549316072021, 0.881]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='green'>Exploration Exercises</font></h2>\n",
    "\n",
    "<p>adding more Neurons we have to do more calculations, slowing down the process, but in this case they have a good impact -- we do get more accurate.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.8790658e-21 0.0000000e+00 9.0761132e-31 0.0000000e+00 0.0000000e+00\n",
      " 4.7847071e-17 0.0000000e+00 2.1176593e-14 1.1832438e-28 1.0000000e+00]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "classifications = model.predict(test_images)\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions represents the probability for the item to be classified as each one of the 10 possible labels.\n",
    "\n",
    "The output of the model is a list of 10 numbers. These numbers are a probability that the value being classified is the corresponding value, i.e. the first value in the list is the probability that the handwriting is of a '0', the next is a '1' etc. Notice that they are all VERY LOW probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.4720\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 12s 199us/sample - loss: 0.3577\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.3220\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 12s 195us/sample - loss: 0.2975\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 12s 206us/sample - loss: 0.2787\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 12s 199us/sample - loss: 0.2627\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 12s 207us/sample - loss: 0.2505\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 13s 217us/sample - loss: 0.2384\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 15s 247us/sample - loss: 0.2282\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 13s 215us/sample - loss: 0.2207\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 12s 201us/sample - loss: 0.2108\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 13s 220us/sample - loss: 0.2011\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 13s 215us/sample - loss: 0.1958\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 13s 212us/sample - loss: 0.1882\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 15s 255us/sample - loss: 0.1843\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 13s 225us/sample - loss: 0.1763\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 13s 220us/sample - loss: 0.1685\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 15s 248us/sample - loss: 0.1633\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 14s 226us/sample - loss: 0.1603\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 14s 228us/sample - loss: 0.1557\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 14s 227us/sample - loss: 0.1505\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 13s 215us/sample - loss: 0.1459\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 13s 209us/sample - loss: 0.1417\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 13s 213us/sample - loss: 0.1410\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 14s 233us/sample - loss: 0.1349\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 14s 229us/sample - loss: 0.1321\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 14s 231us/sample - loss: 0.1295\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 13s 224us/sample - loss: 0.1262\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 14s 228us/sample - loss: 0.1204\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 15s 243us/sample - loss: 0.1179\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 14s 236us/sample - loss: 0.1151\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 13s 223us/sample - loss: 0.1124\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 14s 240us/sample - loss: 0.1126\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 14s 229us/sample - loss: 0.1120\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 13s 215us/sample - loss: 0.1068\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 13s 213us/sample - loss: 0.1045\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 14s 234us/sample - loss: 0.1006\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 13s 219us/sample - loss: 0.0989\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 13s 220us/sample - loss: 0.0961\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 14s 229us/sample - loss: 0.1005\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 13s 224us/sample - loss: 0.0927\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 14s 241us/sample - loss: 0.0921\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 13s 220us/sample - loss: 0.0926\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 13s 210us/sample - loss: 0.0880\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 12s 200us/sample - loss: 0.0868\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 14s 228us/sample - loss: 0.0883\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 12s 201us/sample - loss: 0.0840\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 13s 223us/sample - loss: 0.0821\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 15s 252us/sample - loss: 0.0815\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 13s 216us/sample - loss: 0.0821\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 14s 236us/sample - loss: 0.0776\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 13s 225us/sample - loss: 0.0752\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 12s 207us/sample - loss: 0.0763\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 13s 224us/sample - loss: 0.0771\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 13s 222us/sample - loss: 0.0724\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 16s 275us/sample - loss: 0.0742\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 15s 258us/sample - loss: 0.0710\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 18s 303us/sample - loss: 0.0705\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 15s 249us/sample - loss: 0.0681\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 13s 217us/sample - loss: 0.0687\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 12s 208us/sample - loss: 0.0669\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 12s 205us/sample - loss: 0.0639\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 14s 235us/sample - loss: 0.0634\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 13s 212us/sample - loss: 0.0646\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 13s 218us/sample - loss: 0.0631\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 13s 224us/sample - loss: 0.0609\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 12s 207us/sample - loss: 0.0614\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 14s 229us/sample - loss: 0.0571\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.0595\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 12s 197us/sample - loss: 0.0594\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 12s 200us/sample - loss: 0.0581\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 13s 210us/sample - loss: 0.0544\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 15s 243us/sample - loss: 0.0607\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 13s 223us/sample - loss: 0.0550\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 13s 222us/sample - loss: 0.0571\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 14s 229us/sample - loss: 0.0495\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 15s 247us/sample - loss: 0.0542\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 16s 272us/sample - loss: 0.0566\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 13s 224us/sample - loss: 0.0525\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 13s 222us/sample - loss: 0.0503\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 12s 204us/sample - loss: 0.0520\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 12s 201us/sample - loss: 0.0510\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 13s 222us/sample - loss: 0.0498\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 13s 210us/sample - loss: 0.0454\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 12s 201us/sample - loss: 0.0482\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 12s 204us/sample - loss: 0.0474\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 12s 202us/sample - loss: 0.0464\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 12s 204us/sample - loss: 0.0470\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 12s 206us/sample - loss: 0.0447\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 12s 198us/sample - loss: 0.0480\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 12s 204us/sample - loss: 0.0421\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 12s 204us/sample - loss: 0.0459\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 12s 202us/sample - loss: 0.0493\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 12s 197us/sample - loss: 0.0413\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 12s 199us/sample - loss: 0.0451\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 12s 195us/sample - loss: 0.0477\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.0398\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 11s 189us/sample - loss: 0.0432\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0440\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.0414\n",
      "10000/10000 [==============================] - 1s 87us/sample - loss: 0.9329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9328931479603052"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 88us/sample - loss: 0.9329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9328931479603052"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.8790658e-21 0.0000000e+00 9.0761132e-31 0.0000000e+00 0.0000000e+00\n",
      " 4.7847071e-17 0.0000000e+00 2.1176593e-14 1.1832438e-28 1.0000000e+00]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "classification = model.predict(test_images)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='green'>Removing the Flatten layer</font></h3>\n",
    "\n",
    "<p><font color='red'>InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [896,10] and labels shape [32]\n",
    "\t [[{{node loss_5/output_1_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]</font></p>\n",
    "     \n",
    "<p>You get an error about the shape of the data, this reinforces the rule of the thumb, that <b>the first layer in the network should be the same shape as your data.</b></p>\n",
    " \n",
    "<p>Right now our data is <b>28x28</b> images, and 28 layers of 28 neurons would be <b>infeasible</b>, so it makes more sense to 'flatten' that 28,28 into a <b>784x1</b>.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must have the same first dimension, got logits shape [896,10] and labels shape [32]\n\t [[{{node loss_5/output_1_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-db1f53f78d49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: logits and labels must have the same first dimension, got logits shape [896,10] and labels shape [32]\n\t [[{{node loss_5/output_1_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    #tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='green'>Final (output) Layers</font></h3>\n",
    "\n",
    "<p>Why are there 10 of them? What would happen if you had a different amount than 10? For example, try training the network with 5\n",
    "You get an error as soon as it finds an unexpected value. Another rule of thumb -- the number of neurons in the last layer should match the number of classes you are classifying for.</p>\n",
    "\n",
    "<p>In this case it's the digits 0-9, so there are 10 of them, hence you should have 10 neurons in your final layer.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color='blue'>Callbacks</font></h1>\n",
    "\n",
    "<p> A <b>callback</b> is a function that can be invoked in the process of modle training (<b>fit</b>), say you reach the desired <b>loss</b> on the 30th <b>epoch</b> but you setted 100 <b>epochs</b>, you can call a function to terminate the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('acc') > 0.84):\n",
    "            print('\\nReached 90% accuracy, so cancelling the training!')\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 3.9945 - acc: 0.7528\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.5656 - acc: 0.7977\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.5307 - acc: 0.8157\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.5175 - acc: 0.8214\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.5014 - acc: 0.8271\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 9s 149us/sample - loss: 0.4955 - acc: 0.8314\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.4798 - acc: 0.8351\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 8s 127us/sample - loss: 0.4766 - acc: 0.8381\n",
      "Epoch 9/20\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.4597 - acc: 0.8415\n",
      "Reached 90% accuracy, so cancelling the training!\n",
      "60000/60000 [==============================] - 9s 144us/sample - loss: 0.4595 - acc: 0.8416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c1d64d1a58>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color='blue'>Exercise 2 - Handwriting Digits</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class accuracyCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('acc') >=0.99):\n",
    "            print('\\nReached 99% accuracy so cancelling training!')\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = accuracyCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 143us/sample - loss: 0.2002 - acc: 0.9406\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 127us/sample - loss: 0.0792 - acc: 0.9753\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.0527 - acc: 0.9833\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 155us/sample - loss: 0.0358 - acc: 0.9878\n",
      "Epoch 5/5\n",
      "59584/60000 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9920\n",
      "Reached 99% accuracy so cancelling training!\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0254 - acc: 0.9920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c1d56a6f98>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
